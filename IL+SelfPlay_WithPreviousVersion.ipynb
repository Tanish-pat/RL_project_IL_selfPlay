{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9_JRTFvCr50"
      },
      "source": [
        "# Reinforcement Learning with Imitation Learning and Self-Play\n",
        "\n",
        "The complete workflow for training a Lunar Lander agent using:\n",
        "1. Expert DQN training\n",
        "2. Expert trajectory generation\n",
        "3. Behavioral Cloning\n",
        "4. DAgger (Dataset Aggregation)\n",
        "5. Self-Play enhancement\n",
        "6. Comprehensive evaluation\n",
        "\n",
        "We'll be using the LunarLander-v3 environment from Gymnasium."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62me2486Cr54"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuE8ecQcFFnm"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/content.zip\"\n",
        "extract_path = \"/\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-05-06T04:47:46.183322Z",
          "iopub.status.busy": "2025-05-06T04:47:46.182718Z",
          "iopub.status.idle": "2025-05-06T04:47:46.18743Z",
          "shell.execute_reply": "2025-05-06T04:47:46.186641Z",
          "shell.execute_reply.started": "2025-05-06T04:47:46.183291Z"
        },
        "id": "IAC2TV5kCr55",
        "outputId": "69313ae1-4685-47ec-f03c-6af04f8f8822",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!apt-get install -y swig\n",
        "\n",
        "!git clone https://github.com/openai/box2d-py\n",
        "%cd box2d-py\n",
        "!pip install -e .\n",
        "\n",
        "%cd ..\n",
        "!pip install gymnasium[box2d] --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-05-06T05:15:55.106525Z",
          "iopub.status.busy": "2025-05-06T05:15:55.105725Z",
          "iopub.status.idle": "2025-05-06T05:15:59.61043Z",
          "shell.execute_reply": "2025-05-06T05:15:59.609533Z",
          "shell.execute_reply.started": "2025-05-06T05:15:55.106495Z"
        },
        "id": "6cZnKvgZCr56",
        "outputId": "1f787005-1f0b-457c-a412-5dfbbf25b0cf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install torch matplotlib numpy tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-05-06T04:49:10.126251Z",
          "iopub.status.busy": "2025-05-06T04:49:10.125854Z",
          "iopub.status.idle": "2025-05-06T04:49:14.632631Z",
          "shell.execute_reply": "2025-05-06T04:49:14.631907Z",
          "shell.execute_reply.started": "2025-05-06T04:49:10.126197Z"
        },
        "id": "ucZeLhg1Cr57",
        "outputId": "3f3a6da9-2875-4e7d-8787-046a720d7df0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import pickle\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGgM6QuRCr58"
      },
      "source": [
        "## 2. Define DQN Architecture\n",
        "\n",
        "First, let's define the neural network architecture for our DQN agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-06T04:49:14.633966Z",
          "iopub.status.busy": "2025-05-06T04:49:14.63353Z",
          "iopub.status.idle": "2025-05-06T04:49:14.639255Z",
          "shell.execute_reply": "2025-05-06T04:49:14.638483Z",
          "shell.execute_reply.started": "2025-05-06T04:49:14.633937Z"
        },
        "id": "dGlVLAKVCr58",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(n_observations, 128)\n",
        "        self.layer2 = nn.Linear(128, 128)\n",
        "        self.layer3 = nn.Linear(128, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.layer3(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6HvOBuzCr59"
      },
      "source": [
        "## 3. DQN Expert Training\n",
        "\n",
        "Here we'll train our expert DQN model on the LunarLander environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-06T04:49:14.641659Z",
          "iopub.status.busy": "2025-05-06T04:49:14.641191Z",
          "iopub.status.idle": "2025-05-06T04:49:14.658961Z",
          "shell.execute_reply": "2025-05-06T04:49:14.658173Z",
          "shell.execute_reply.started": "2025-05-06T04:49:14.64164Z"
        },
        "id": "RkO7mMnhCr59",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "num_episodes = 800\n",
        "batch_size = 128\n",
        "GAMMA = 0.99\n",
        "LR = 1e-4\n",
        "TAU = 0.005\n",
        "\n",
        "EPSILON = 1.0  # start with full exploration\n",
        "EPSILON_MIN = 0.01  # minimum value\n",
        "EPSILON_DECAY = 0.995  # decay factor per episode\n",
        "\n",
        "reward_list = []\n",
        "episode_durations = []\n",
        "\n",
        "# transition namedtuple to store the trajectories\n",
        "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"next_state\", \"reward\", \"done\"])\n",
        "\n",
        "# Replay Memory Buffer\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return (\n",
        "            random.sample(self.memory, batch_size)\n",
        "            if batch_size < len(self.memory)\n",
        "            else self.memory\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-06T04:58:26.776728Z",
          "iopub.status.busy": "2025-05-06T04:58:26.77599Z",
          "iopub.status.idle": "2025-05-06T04:58:26.842014Z",
          "shell.execute_reply": "2025-05-06T04:58:26.840877Z",
          "shell.execute_reply.started": "2025-05-06T04:58:26.776701Z"
        },
        "id": "QcAsn3MqCr5-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# initialize the environment\n",
        "env = gym.make(\"LunarLander-v3\")\n",
        "\n",
        "n_observations = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "policy_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "replay_memory = ReplayMemory(10000)\n",
        "\n",
        "def select_action(state, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return torch.tensor(\n",
        "            [[env.action_space.sample()]], dtype=torch.long, device=device\n",
        "        )\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state).max(1).indices.view(1, 1)  # Exploit (best action)\n",
        "\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR)\n",
        "criterion = nn.SmoothL1Loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d00982750c094b32a3a5c64be2aec880",
            "f4bcc5a44fbe4e198a1ffa0e33a495cf",
            "fbf2362d237e4646a9e988702d765d15",
            "56cdbf5594524beeacf154057f3fb53a",
            "31d602e4e17b4cbdaec861c4d3b62afa",
            "3cd00882062241d6b652abcec5761655",
            "3ce8a41694cf43fdb5dd5f3ed2745edc",
            "bbd5c1788f3543518c7b4c65cd9518a1",
            "4e447f30d60d4be8b27fb8a35ed62b77",
            "23cc1890aa204a38872648920d4c4b6f",
            "0195f6a3f23c4412b9833d45f8bab700"
          ]
        },
        "execution": {
          "iopub.status.busy": "2025-05-06T04:49:14.701303Z",
          "iopub.status.idle": "2025-05-06T04:49:14.701514Z",
          "shell.execute_reply": "2025-05-06T04:49:14.701424Z",
          "shell.execute_reply.started": "2025-05-06T04:49:14.701415Z"
        },
        "id": "PSF9hf4WCr5-",
        "outputId": "c8bfcc85-205c-4093-81ad-029764385481",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# training the expert DQN model\n",
        "print(\"Training Expert DQN...\")\n",
        "\n",
        "for episode in tqdm(range(num_episodes), desc=\"Training Episodes\"):\n",
        "    state, info = env.reset()\n",
        "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    total_reward = 0\n",
        "\n",
        "    for t in count():\n",
        "        action = select_action(state, EPSILON)\n",
        "        next_state, reward, terminated, truncated, info = env.step(action.item())\n",
        "\n",
        "        done = terminated or truncated\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        next_state = torch.tensor(\n",
        "            next_state, dtype=torch.float32, device=device\n",
        "        ).unsqueeze(0)\n",
        "        replay_memory.push(state, action, next_state, reward, done)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward.item()\n",
        "\n",
        "        if len(replay_memory) >= batch_size:\n",
        "            transitions = replay_memory.sample(batch_size)\n",
        "            states, actions, next_states, rewards, dones = zip(*transitions)\n",
        "\n",
        "            states_batch = torch.cat(states)\n",
        "            next_states_batch = torch.cat(next_states)\n",
        "            actions_batch = torch.cat(actions)\n",
        "            rewards = torch.tensor(rewards, device=device)\n",
        "            dones = torch.tensor(dones, device=device)\n",
        "\n",
        "            q_target = (\n",
        "                GAMMA * target_net(next_states_batch).detach().max(1)[0] * ~dones\n",
        "                + rewards\n",
        "            )\n",
        "            q_policy = policy_net(states_batch).gather(1, actions_batch)\n",
        "\n",
        "            # calculating the Huber loss\n",
        "            loss = criterion(q_policy, q_target.unsqueeze(1))\n",
        "\n",
        "            # optimize the model\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # in-place gradient clipping to stabilize training\n",
        "            torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        # update target network\n",
        "        for target_param, main_param in zip(\n",
        "            target_net.parameters(), policy_net.parameters()\n",
        "        ):\n",
        "            target_param.data.copy_(\n",
        "                TAU * main_param.data + (1 - TAU) * target_param.data\n",
        "            )\n",
        "\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            reward_list.append(total_reward)\n",
        "            break\n",
        "\n",
        "    # Decay epsilon\n",
        "    EPSILON = max(EPSILON_MIN, EPSILON * EPSILON_DECAY)\n",
        "\n",
        "    if episode % 10 == 0:\n",
        "        avg_reward = np.mean(reward_list[-10:]) if reward_list else 0\n",
        "        print(f\"Episode {episode}/{num_episodes}, Avg Reward: {avg_reward:.2f}, Epsilon: {EPSILON:.2f}\")\n",
        "\n",
        "torch.save(policy_net.state_dict(), \"models/dqn_lunar_lander.pth\")\n",
        "print(\"Expert DQN model saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLm2OduFR7KQ"
      },
      "source": [
        "1. Reward Trend per Episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "ko-nETaXRx28",
        "outputId": "becab1ff-d0cd-4963-b4de-26ae4926cb61"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(reward_list, label=\"Episode Reward\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"DQN: Episode Reward over Time\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzxTf3_XSAwS"
      },
      "source": [
        "2. Smoothed Reward Curve (Moving Average)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "ynkmNOuSSBjj",
        "outputId": "0aa12750-78d8-4eac-d320-f787e8218a11"
      },
      "outputs": [],
      "source": [
        "def moving_average(data, window_size=10):\n",
        "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(moving_average(reward_list), label=\"Smoothed Reward (10-episode MA)\", color=\"orange\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"DQN: Smoothed Episode Rewards\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2hQLfFgSIj2"
      },
      "source": [
        "3. Epsilon Decay Over Episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "hnlOVJaCSJNH",
        "outputId": "c5d449f2-3cdd-407e-83e9-55ff01b063e2"
      },
      "outputs": [],
      "source": [
        "epsilons = [max(EPSILON_MIN, EPSILON * (EPSILON_DECAY ** i)) for i in range(num_episodes)]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epsilons, label=\"Epsilon Value\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Epsilon\")\n",
        "plt.title(\"Epsilon Decay Over Time\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfLzotuzCr5_"
      },
      "source": [
        "## 4. Generate Expert Trajectories\n",
        "\n",
        "Let's generate and save expert trajectories using our trained DQN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2025-05-06T04:49:14.703127Z",
          "iopub.status.idle": "2025-05-06T04:49:14.703518Z",
          "shell.execute_reply": "2025-05-06T04:49:14.703349Z",
          "shell.execute_reply.started": "2025-05-06T04:49:14.70333Z"
        },
        "id": "XU3SaMUyCr5_",
        "outputId": "ed167a46-6857-4394-bf38-8dd72c33ab3a",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Saving the videos of the expert model stimulation\n",
        "video_folder = \"./dqn_expert_videos/\"\n",
        "os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "expert_policy = DQN(n_observations, n_actions).to(device)\n",
        "expert_policy.load_state_dict(torch.load(\"models/dqn_lunar_lander.pth\", map_location=device))\n",
        "expert_policy.eval()\n",
        "\n",
        "# configuring environment for video recording\n",
        "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "env = gym.wrappers.RecordVideo(\n",
        "    env,\n",
        "    video_folder,\n",
        "    episode_trigger=lambda ep: ep < 2,  # saving only first 2 episodes\n",
        "    name_prefix=\"dqn_expert\"\n",
        ")\n",
        "\n",
        "# expert trajectories\n",
        "expert_trajectories = []\n",
        "n_episodes = 30\n",
        "\n",
        "print(\"Generating expert trajectories...\")\n",
        "for episode in range(n_episodes):\n",
        "    state, _ = env.reset()\n",
        "    state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "    trajectory = {\"obs\": [], \"actions\": [], \"rewards\": []}\n",
        "    total_reward = 0\n",
        "\n",
        "    for t in count():\n",
        "        with torch.no_grad():\n",
        "            action = expert_policy(state_tensor).argmax(dim=1).item()\n",
        "\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        trajectory[\"obs\"].append(state)\n",
        "        trajectory[\"actions\"].append(action)\n",
        "        trajectory[\"rewards\"].append(reward)\n",
        "\n",
        "        total_reward += reward\n",
        "        done = terminated or truncated\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        state = next_state\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "    expert_trajectories.append(trajectory)\n",
        "    print(f\"Episode {episode + 1}/{n_episodes} | Total reward: {total_reward:.2f}\")\n",
        "\n",
        "env.close()\n",
        "print(f\"Saved videos to: {video_folder}\")\n",
        "\n",
        "with open('expert_trajectories.pkl', 'wb') as f:\n",
        "    pickle.dump(expert_trajectories, f)\n",
        "print(\"Expert trajectories saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_Lute68TMFO"
      },
      "source": [
        "1. Total Reward per Expert Episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HiR2T6ETLhZ"
      },
      "outputs": [],
      "source": [
        "expert_rewards = [sum(traj[\"rewards\"]) for traj in expert_trajectories]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(range(1, len(expert_rewards) + 1), expert_rewards, color='skyblue')\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"Total Reward per Expert Episode\")\n",
        "plt.xticks(range(1, len(expert_rewards) + 1))\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqjIsnMKTLX7"
      },
      "source": [
        "2. Action Distribution Over Expert Trajectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vt_AwvGrTLP5"
      },
      "outputs": [],
      "source": [
        "all_actions = [action for traj in expert_trajectories for action in traj[\"actions\"]]\n",
        "action_counts = [all_actions.count(a) for a in range(n_actions)]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(range(n_actions), action_counts, color='lightgreen')\n",
        "plt.xlabel(\"Action\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Actions Taken by Expert\")\n",
        "plt.xticks(range(n_actions))\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsocD9TGTLHR"
      },
      "source": [
        "3. Cumulative Reward Curve for All Episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM9SfECyTK-o"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "for i, traj in enumerate(expert_trajectories):\n",
        "    cum_reward = np.cumsum(traj[\"rewards\"])\n",
        "    plt.plot(cum_reward, label=f\"Episode {i+1}\")\n",
        "\n",
        "plt.xlabel(\"Timestep\")\n",
        "plt.ylabel(\"Cumulative Reward\")\n",
        "plt.title(\"Cumulative Reward over Time per Expert Episode\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u9yoaOGTKxs"
      },
      "source": [
        "4. Reward Density Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "Qz1tCNK1TzXN",
        "outputId": "8fab8947-c247-47a6-e8b1-17257d2c8954"
      },
      "outputs": [],
      "source": [
        "all_rewards = [r for traj in expert_trajectories for r in traj[\"rewards\"]]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(all_rewards, bins=30, color=\"coral\", edgecolor=\"black\", alpha=0.75)\n",
        "plt.xlabel(\"Reward\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Reward Distribution Across All Expert Steps\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpUUWqErCr6A"
      },
      "source": [
        "## 5. Behavioral Cloning\n",
        "\n",
        "Train a student model to mimic the expert's behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431,
          "referenced_widgets": [
            "8b8fde6236924faa88e5c7338c4a0696",
            "5def1efbc04449e0a055b996d075f20a",
            "402e82e6276444e6910089f5af342f7b",
            "9d29573c53e44d0983b855c1567bacf3",
            "6359400387974a5798293f21df4ba5ad",
            "82ed5c7f2f17497f9adaf94abe3beaf4",
            "17267cd5c7e2463693ef42d8353dfbce",
            "44923cd3136b4079933948325ea49512",
            "66a053ea81f6415995880111cf2b3fca",
            "f868ca4da37f4b5091f37b51a45ba3d3",
            "0c9bf07b0078468fa833ab9111784007"
          ]
        },
        "execution": {
          "iopub.status.busy": "2025-05-06T04:49:14.704734Z",
          "iopub.status.idle": "2025-05-06T04:49:14.705043Z",
          "shell.execute_reply": "2025-05-06T04:49:14.704905Z",
          "shell.execute_reply.started": "2025-05-06T04:49:14.70489Z"
        },
        "id": "HtP40XtOCr6A",
        "outputId": "0f1bf3da-fa7b-4303-8aad-01ebe3eab35e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "with open('expert_trajectories.pkl', 'rb') as f:\n",
        "    expert_trajectories = pickle.load(f)\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "# Student Model (same architecture as DQN but fresh weights)\n",
        "env = gym.make(\"LunarLander-v3\")\n",
        "n_observations = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "student_model = DQN(n_observations, n_actions).to(device)\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# dataset from expert trajectories\n",
        "states = []\n",
        "actions = []\n",
        "for trajectory in expert_trajectories:\n",
        "    states.extend(trajectory['obs'])\n",
        "    actions.extend(trajectory['actions'])\n",
        "\n",
        "states = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
        "actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
        "\n",
        "# Training loop\n",
        "epochs = 200\n",
        "batch_size = 64\n",
        "dataset_size = len(states)\n",
        "\n",
        "print(\"Training student model with Behavioral Cloning...\")\n",
        "for epoch in tqdm(range(epochs), desc=\"BC Training\"):\n",
        "    # shuffle data\n",
        "    indices = torch.randperm(dataset_size)\n",
        "    total_loss = 0.0\n",
        "    batches = 0\n",
        "\n",
        "    for start_idx in range(0, dataset_size, batch_size):\n",
        "        # getting batch data acc to batch size and indices\n",
        "        batch_indices = indices[start_idx:start_idx + batch_size]\n",
        "\n",
        "        state_batch = states[batch_indices]\n",
        "        action_batch = actions[batch_indices]\n",
        "\n",
        "        # Forward pass\n",
        "        logits = student_model(state_batch)\n",
        "        loss = criterion(logits, action_batch)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        batches += 1\n",
        "\n",
        "    avg_loss = total_loss / batches\n",
        "    loss_history.append(avg_loss)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "torch.save(student_model.state_dict(), \"models/imitation_model.pth\")\n",
        "print(\"Behavioral Cloning model saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FtXvg3vUBVL"
      },
      "source": [
        "1. Loss Curve During Behavioral Cloning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM1ph_KZUBMJ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_history, color='royalblue', marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Cross-Entropy Loss\")\n",
        "plt.title(\"Student Model Loss Curve (Behavioral Cloning)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-qdNPaUUBFn"
      },
      "source": [
        "2. Student vs Expert Action Match (Accuracy Estimate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmTr59Y-UA69"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    student_logits = student_model(states)\n",
        "    student_preds = student_logits.argmax(dim=1)\n",
        "\n",
        "    correct = (student_preds == actions).sum().item()\n",
        "    total = len(actions)\n",
        "    incorrect = total - correct\n",
        "    accuracy = correct / total * 100\n",
        "\n",
        "# Enhanced plot\n",
        "fig, ax = plt.subplots(figsize=(7, 5))\n",
        "\n",
        "bars = ax.bar([\"Correct\", \"Incorrect\"], [correct, incorrect], color=[\"seagreen\", \"indianred\"], edgecolor='black')\n",
        "\n",
        "# Add value labels on top of each bar\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width() / 2, yval + total * 0.01, f\"{yval}\", ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "ax.set_title(f\"Student Model Action Matching\\nAccuracy: {accuracy:.2f}%\", fontsize=14, weight='bold')\n",
        "ax.set_ylabel(\"Number of Actions\", fontsize=12)\n",
        "ax.tick_params(axis='both', labelsize=11)\n",
        "ax.set_ylim(0, total * 1.1)  # add some space above bars\n",
        "ax.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA3TN29dUAx1"
      },
      "source": [
        "3. Action Distribution: Expert vs Student"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oc9h5UnUAot"
      },
      "outputs": [],
      "source": [
        "# Expert actions\n",
        "expert_action_counts = [actions.cpu().tolist().count(i) for i in range(n_actions)]\n",
        "\n",
        "# Student actions\n",
        "student_action_counts = [student_preds.cpu().tolist().count(i) for i in range(n_actions)]\n",
        "\n",
        "x = range(n_actions)\n",
        "bar_width = 0.35\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(x, expert_action_counts, width=bar_width, label='Expert', color='skyblue')\n",
        "plt.bar([i + bar_width for i in x], student_action_counts, width=bar_width, label='Student', color='orange')\n",
        "plt.xlabel(\"Action\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Action Distribution: Expert vs Student\")\n",
        "plt.xticks([i + bar_width/2 for i in x], x)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZXjWJ2hCr6A"
      },
      "source": [
        "## 6. DAgger (Dataset Aggregation)\n",
        "\n",
        "Use DAgger to improve the student model by aggregating additional expert-labeled data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-06T04:49:14.705739Z",
          "iopub.status.idle": "2025-05-06T04:49:14.705946Z",
          "shell.execute_reply": "2025-05-06T04:49:14.705858Z",
          "shell.execute_reply.started": "2025-05-06T04:49:14.705849Z"
        },
        "id": "YydeBKJhCr6A",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\")\n",
        "n_observations = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# Expert\n",
        "expert_model = DQN(n_observations, n_actions).to(device)\n",
        "expert_model.load_state_dict(torch.load(\"models/dqn_lunar_lander.pth\", map_location=device))\n",
        "expert_model.eval()\n",
        "\n",
        "# Student (initially from behavioral cloning)\n",
        "student_model = DQN(n_observations, n_actions).to(device)\n",
        "student_model.load_state_dict(torch.load(\"models/imitation_model.pth\", map_location=device))\n",
        "\n",
        "with open('dagger_dataset.pkl', 'rb') as f:\n",
        "    dataset = pickle.load(f)\n",
        "    print(\"Loaded DAgger dataset\")\n",
        "\n",
        "# DAgger parameters\n",
        "n_dagger_iterations = 5\n",
        "n_episodes_per_iteration = 10\n",
        "epochs_per_iteration = 20\n",
        "batch_size = 64\n",
        "\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-06T04:49:14.707271Z",
          "iopub.status.idle": "2025-05-06T04:49:14.70753Z",
          "shell.execute_reply": "2025-05-06T04:49:14.70743Z",
          "shell.execute_reply.started": "2025-05-06T04:49:14.707419Z"
        },
        "id": "pdVz-IJECr6B",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# === Tracking lists ===\n",
        "losses_per_iteration = []\n",
        "dataset_sizes = []\n",
        "\n",
        "# DAgger training loop\n",
        "print(\"Starting DAgger training...\")\n",
        "for dagger_iter in range(n_dagger_iterations):\n",
        "    print(f\"\\nDAgger Iteration {dagger_iter+1}/{n_dagger_iterations}\")\n",
        "\n",
        "    # Collect data with student policy, but label with expert\n",
        "    for episode in tqdm(range(n_episodes_per_iteration), desc=\"Collecting data\"):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Student collects state\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "            # Student takes action\n",
        "            with torch.no_grad():\n",
        "                student_action = student_model(state_tensor).argmax(dim=1).item()\n",
        "\n",
        "            # Expert labels the state\n",
        "            with torch.no_grad():\n",
        "                expert_action = expert_model(state_tensor).argmax(dim=1).item()\n",
        "\n",
        "            # Add to dataset with expert label\n",
        "            dataset['states'].append(state)\n",
        "            dataset['actions'].append(expert_action)\n",
        "\n",
        "            # Execute student's action to get next state\n",
        "            next_state, _, terminated, truncated, _ = env.step(student_action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "\n",
        "    # Convert to tensors\n",
        "    states = torch.tensor(np.array(dataset['states']), dtype=torch.float32).to(device)\n",
        "    actions = torch.tensor(dataset['actions'], dtype=torch.long).to(device)\n",
        "\n",
        "    # Train student on aggregated dataset\n",
        "    dataset_size = len(states)\n",
        "    print(f\"Training on aggregated dataset of size {dataset_size}...\")\n",
        "    dataset_sizes.append(dataset_size)\n",
        "\n",
        "    avg_epoch_losses = []\n",
        "\n",
        "    for epoch in tqdm(range(epochs_per_iteration), desc=\"Training\"):\n",
        "        # Shuffle data\n",
        "        indices = torch.randperm(dataset_size)\n",
        "        total_loss = 0.0\n",
        "        batches = 0\n",
        "\n",
        "        for start_idx in range(0, dataset_size, batch_size):\n",
        "            # Get batch indices\n",
        "            batch_indices = indices[start_idx:start_idx + batch_size]\n",
        "\n",
        "            # Get batch data\n",
        "            state_batch = states[batch_indices]\n",
        "            action_batch = actions[batch_indices]\n",
        "\n",
        "            # Forward pass\n",
        "            logits = student_model(state_batch)\n",
        "            loss = criterion(logits, action_batch)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batches += 1\n",
        "\n",
        "        avg_loss = total_loss / batches\n",
        "        avg_epoch_losses.append(avg_loss)\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            avg_loss = total_loss / batches\n",
        "            print(f\"  Epoch {epoch+1}/{epochs_per_iteration}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    losses_per_iteration.append(avg_epoch_losses)\n",
        "\n",
        "    # Save the updated model\n",
        "    torch.save(student_model.state_dict(), f\"models/dagger_model_iter{dagger_iter+1}.pth\")\n",
        "\n",
        "    # Save the dataset\n",
        "    with open('dagger_dataset.pkl', 'wb') as f:\n",
        "        pickle.dump(dataset, f)\n",
        "\n",
        "print(\"DAgger training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW3Wj-cHWN_U"
      },
      "source": [
        "Plot 1: Loss curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWFFz0sWWNz-"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(10, 6))\n",
        "# for i, losses in enumerate(losses_per_iteration):\n",
        "#     plt.plot(losses, label=f\"DAgger Iter {i+1}\")\n",
        "# plt.title(\"Student Loss per Epoch During DAgger Iterations\", fontsize=14, weight='bold')\n",
        "# plt.xlabel(\"Epoch\", fontsize=12)\n",
        "# plt.ylabel(\"Cross-Entropy Loss\", fontsize=12)\n",
        "# plt.legend()\n",
        "# plt.grid(True, linestyle='--', alpha=0.5)\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, losses in enumerate(losses_per_iteration):\n",
        "    plt.plot(losses, label=f\"DAgger Iter {i+1}\")\n",
        "plt.title(\"Student Log Loss per Epoch During DAgger Iterations\", fontsize=14, weight='bold')\n",
        "plt.xlabel(\"Epoch\", fontsize=12)\n",
        "plt.ylabel(\"Cross-Entropy Loss (Log Scale)\", fontsize=12)\n",
        "plt.yscale(\"log\")  # Apply log scale here\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5, which='both')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg8coKUzWNrn"
      },
      "source": [
        "Plot 2: Dataset growth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "xC5YYnGPWNjW",
        "outputId": "85c6db1e-a2bc-4328-b66b-87b985759157"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, n_dagger_iterations + 1), dataset_sizes, marker='o', color='teal', linewidth=2)\n",
        "for i, size in enumerate(dataset_sizes):\n",
        "    plt.text(i + 1, size, str(size), ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.title(\"Aggregated Dataset Size After Each DAgger Iteration\", fontsize=14, weight='bold')\n",
        "plt.xlabel(\"DAgger Iteration\", fontsize=12)\n",
        "plt.ylabel(\"Total Samples\", fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.xticks(range(1, n_dagger_iterations + 1))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUSejNcbCr6B"
      },
      "source": [
        "## 7. Self-Play Enhancement\n",
        "\n",
        "Further improve the model through self-play reinforcement learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AulEFvJREK1I"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "MEMORY_SIZE = 100000\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "TAU = 0.005\n",
        "LR = 0.0001\n",
        "N_ITERATIONS = 50\n",
        "EPISODES_PER_ITER = 20\n",
        "ALPHA = 0.7  # Weight for imitation loss\n",
        "BETA = 0.2   # Weight for policy divergence\n",
        "GAMMA_REWARD = 0.1  # Weight for reward optimization\n",
        "\n",
        "# *** Added: container for average loss metrics per iteration\n",
        "loss_history = []\n",
        "all_opps = set()\n",
        "\n",
        "def evaluate_against_opponent(model, opponent_model, env, episodes=5):\n",
        "    \"\"\"Evaluate how model performs when competing against an opponent version\"\"\"\n",
        "    model.eval()\n",
        "    opponent_model.eval()\n",
        "\n",
        "    # In Lunar Lander single-agent, we simulate \"competition\" by comparing performance\n",
        "    # on the same initial states\n",
        "    total_rewards_model = []\n",
        "    total_rewards_opponent = []\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        # Initialize with same random seed\n",
        "        seed = random.randint(0, 10000)\n",
        "\n",
        "        # First let model play\n",
        "        state, _ = env.reset(seed=seed)\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                action = model(state_tensor).argmax(dim=1).item()\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "\n",
        "        total_rewards_model.append(total_reward)\n",
        "\n",
        "        # Then let opponent play same scenario\n",
        "        state, _ = env.reset(seed=seed)\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                action = opponent_model(state_tensor).argmax(dim=1).item()\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "\n",
        "        total_rewards_opponent.append(total_reward)\n",
        "\n",
        "    model.train()\n",
        "    win_rate = sum(r_m > r_o for r_m, r_o in zip(total_rewards_model, total_rewards_opponent)) / episodes\n",
        "\n",
        "    return {\n",
        "        \"model_avg\": np.mean(total_rewards_model),\n",
        "        \"opponent_avg\": np.mean(total_rewards_opponent),\n",
        "        \"win_rate\": win_rate\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_competitive_reward(state, next_state, reward, model_action, opponent_action, opponent_type, done):\n",
        "    # Base environmental reward\n",
        "    competitive_reward = reward\n",
        "\n",
        "    # Expert alignment reward (more nuanced than current binary approach)\n",
        "    if opponent_type == \"expert\":\n",
        "        # Graduated reward based on Q-value difference between actions\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "            q_values = expert_model(state_tensor)[0]\n",
        "            model_q = q_values[model_action].item()\n",
        "            expert_q = q_values[opponent_action].item()\n",
        "            q_diff = expert_q - model_q\n",
        "\n",
        "            # Scaled reward bonus with diminishing returns\n",
        "            alignment_bonus = 1.0 if model_action == opponent_action else max(-0.5, -0.1 * min(5.0, abs(q_diff)))\n",
        "            competitive_reward += alignment_bonus\n",
        "\n",
        "    # Stability reward - extra reward for keeping lander stable (based on state values)\n",
        "    if not done:\n",
        "        # Extract angular velocity from state\n",
        "        angular_velocity = abs(state[5])\n",
        "        stability_bonus = 0.2 * (1.0 - min(1.0, angular_velocity/0.5))\n",
        "        competitive_reward += stability_bonus\n",
        "\n",
        "    # Landing bonus - large bonus for safe landings\n",
        "    if done and reward > 100:  # Successful landing\n",
        "        competitive_reward += 5.0\n",
        "\n",
        "    return competitive_reward\n",
        "\n",
        "\n",
        "def select_strategic_opponents(model_versions, best_model, n=3):\n",
        "    if len(model_versions) <= n:\n",
        "        return model_versions\n",
        "\n",
        "    # Always include expert\n",
        "    expert = next((m for m in model_versions if m[\"name\"] == \"Expert\"), None)\n",
        "    selected = [expert] if expert else []\n",
        "\n",
        "    # Calculate difficulty scores for each opponent\n",
        "    difficulty_scores = []\n",
        "    for opponent_info in model_versions:\n",
        "        if opponent_info in selected:\n",
        "            continue\n",
        "\n",
        "        # Run a quick evaluation to see how challenging this opponent is\n",
        "        results = quick_evaluate_against(best_model, opponent_info[\"model\"], episodes=2)\n",
        "\n",
        "        # Score based on how close the win rate is to 0.5 (most educational)\n",
        "        challenge_score = 1.0 - abs(results[\"win_rate\"] - 0.5) * 2.0\n",
        "        difficulty_scores.append((opponent_info, challenge_score))\n",
        "\n",
        "    # Select most challenging opponents\n",
        "    difficulty_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    selected.extend([info for info, _ in difficulty_scores[:n-len(selected)]])\n",
        "\n",
        "    return selected\n",
        "\n",
        "\n",
        "def evaluate_policy(model, env, episodes=5):\n",
        "    \"\"\"Evaluate a policy without exploration\"\"\"\n",
        "    model.eval()\n",
        "    total_rewards = []\n",
        "    for _ in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                action = model(state_tensor).argmax(dim=1).item()\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "        total_rewards.append(total_reward)\n",
        "    model.train()\n",
        "    return np.mean(total_rewards)\n",
        "def compute_enhanced_composite_loss(state_action_values, expected_values,\n",
        "                                   logits, expert_actions, prev_logits,\n",
        "                                   states, next_states, actions,\n",
        "                                   alpha=0.7, beta=0.2,  # Using direct values instead of globals\n",
        "                                   batch_size=128):      # Adding batch_size parameter\n",
        "    \"\"\"Enhanced composite loss with RL, imitation, policy regularization, and safety terms\"\"\"\n",
        "\n",
        "    # 1. RL loss (TD error)\n",
        "    rl_loss = F.smooth_l1_loss(state_action_values, expected_values)\n",
        "\n",
        "    # 2. Imitation loss (when expert actions are available)\n",
        "    imitation_loss = torch.tensor(0.0, device=device)\n",
        "    if expert_actions is not None:\n",
        "        expert_logits = logits[batch_size//2:]\n",
        "        imitation_loss = F.cross_entropy(expert_logits, expert_actions)\n",
        "\n",
        "    # 3. Policy regularization (KL divergence)\n",
        "    div_loss = torch.tensor(0.0, device=device)\n",
        "    if prev_logits is not None:\n",
        "        log_probs = F.log_softmax(logits, dim=1)\n",
        "        prev_probs = F.softmax(prev_logits.detach(), dim=1)\n",
        "        div_loss = F.kl_div(log_probs, prev_probs, reduction='batchmean')\n",
        "\n",
        "    # 4. NEW: Safety loss - penalize states that lead to crashes - FIX HERE\n",
        "    safety_loss = torch.tensor(0.0, device=device)\n",
        "    if next_states is not None:\n",
        "        # Identify crashes (strongly negative rewards)\n",
        "        crashes = torch.tensor([r < -50 for r in expected_values], device=device)\n",
        "\n",
        "        if crashes.any():\n",
        "            # Extract Q-values for actions that led to crashes\n",
        "            crash_indices = torch.where(crashes)[0]\n",
        "\n",
        "            # Fix: Ensure action indices have the right shape and type for gather\n",
        "            crash_actions = actions[crash_indices]\n",
        "            if not isinstance(crash_actions, torch.LongTensor) and not isinstance(crash_actions, torch.cuda.LongTensor):\n",
        "                crash_actions = crash_actions.long()\n",
        "\n",
        "            # Gather Q-values for the actions that led to crashes\n",
        "            crash_q_values = logits[crash_indices].gather(1, crash_actions)\n",
        "\n",
        "            # Strongly penalize these actions\n",
        "            safety_target = torch.full_like(crash_q_values, -100.0)\n",
        "            safety_loss = F.mse_loss(crash_q_values, safety_target)\n",
        "\n",
        "    # 5. Entropy bonus for exploration when not learning from expert\n",
        "    entropy_loss = torch.tensor(0.0, device=device)\n",
        "    if expert_actions is None:\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        log_probs = F.log_softmax(logits, dim=1)\n",
        "        entropy = -(probs * log_probs).sum(dim=1).mean()\n",
        "        # Negative because we want to maximize entropy (exploration)\n",
        "        entropy_loss = -0.01 * entropy\n",
        "\n",
        "    # Combine all losses with weights\n",
        "    combined_loss = (rl_loss +\n",
        "                    alpha * imitation_loss +\n",
        "                    beta * div_loss +\n",
        "                    0.5 * safety_loss +\n",
        "                    entropy_loss)\n",
        "\n",
        "    return combined_loss, {\n",
        "        'rl': rl_loss.item(),\n",
        "        'imitation': imitation_loss.item(),\n",
        "        'divergence': div_loss.item(),\n",
        "        'safety': safety_loss.item(),\n",
        "        'entropy': entropy_loss.item()\n",
        "    }\n",
        "\n",
        "# Self-play training with multi-generational competition\n",
        "print(\"Starting enhanced self-play training...\")\n",
        "\n",
        "# Load different generations of models\n",
        "model_versions = []\n",
        "try:\n",
        "    # Load DAgger models\n",
        "    for i in range(1, 6):  # Load all 5 DAgger iterations\n",
        "        model_path = f\"models/dagger_model_iter{i}.pth\"\n",
        "        model = DQN(n_observations, n_actions).to(device)\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        model.eval()  # Set to evaluation mode\n",
        "        model_versions.append({\"model\": model, \"name\": f\"DAgger-{i}\"})\n",
        "    print(f\"Loaded {len(model_versions)} previous model versions\")\n",
        "\n",
        "    # Add expert model as another competitor\n",
        "    expert_model = DQN(n_observations, n_actions).to(device)\n",
        "    expert_model.load_state_dict(torch.load(\"models/dqn_lunar_lander.pth\", map_location=device))\n",
        "    expert_model.eval()\n",
        "    model_versions.append({\"model\": expert_model, \"name\": \"Expert\"})\n",
        "    print(\"Added expert model to competitors\")\n",
        "except Exception as e:\n",
        "    print(f\"Couldn't load all model versions: {e}\")\n",
        "    print(\"Starting with only available models\")\n",
        "\n",
        "# Current best model (student) that we'll continue to improve\n",
        "best_model = DQN(n_observations, n_actions).to(device)\n",
        "try:\n",
        "    best_model.load_state_dict(torch.load(\"models/dagger_model_iter5.pth\", map_location=device))\n",
        "    print(\"Loaded latest DAgger model as starting point\")\n",
        "except:\n",
        "    print(\"Starting with a fresh model\")\n",
        "\n",
        "# Create target model for stable learning\n",
        "target_model = DQN(n_observations, n_actions).to(device)\n",
        "target_model.load_state_dict(best_model.state_dict())\n",
        "target_model.eval()\n",
        "\n",
        "# Memory buffers - one standard and one for expert demonstrations\n",
        "memory = ReplayMemory(MEMORY_SIZE)\n",
        "expert_memory = ReplayMemory(MEMORY_SIZE // 2)  # Smaller buffer for expert demos\n",
        "\n",
        "optimizer = optim.Adam(best_model.parameters(), lr=LR)\n",
        "\n",
        "# Fill expert memory with some expert demonstrations\n",
        "print(\"Collecting expert demonstrations...\")\n",
        "expert_model = DQN(n_observations, n_actions).to(device)\n",
        "expert_model.load_state_dict(torch.load(\"models/dqn_lunar_lander.pth\", map_location=device))\n",
        "expert_model.eval()\n",
        "\n",
        "for _ in tqdm(range(20), desc=\"Expert Demo Collection\"):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            action = expert_model(state_tensor).argmax(dim=1).item()\n",
        "\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # Store expert transition\n",
        "        expert_memory.push(\n",
        "            state,\n",
        "            action,\n",
        "            next_state if not done else None,\n",
        "            reward,\n",
        "            done\n",
        "        )\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "# Performance tracker\n",
        "performance_history = []\n",
        "best_reward = evaluate_policy(best_model, env)\n",
        "print(f\"Initial performance: {best_reward:.2f}\")\n",
        "\n",
        "# Create model history folder\n",
        "os.makedirs(\"models/competition_history\", exist_ok=True)\n",
        "\n",
        "# Main training loop\n",
        "for iteration in range(N_ITERATIONS):\n",
        "    print(f\"\\nIteration {iteration+1}/{N_ITERATIONS}\")\n",
        "\n",
        "    # Use strategic opponent selection instead of random sampling\n",
        "    if model_versions:\n",
        "        # Define the quick evaluation function used by select_strategic_opponents\n",
        "        def quick_evaluate_against(model, opponent_model, episodes=2):\n",
        "            return evaluate_against_opponent(model, opponent_model, env, episodes)\n",
        "\n",
        "        opponents = select_strategic_opponents(model_versions, best_model, n=3)\n",
        "        for opponent in opponents:\n",
        "            print(f\"Competing against: {opponent['name']}\")\n",
        "    else:\n",
        "        opponents = []\n",
        "        print(\"No opponent models available\")\n",
        "\n",
        "    # 1. Collect experience through standard self-play\n",
        "    for episode in tqdm(range(EPISODES_PER_ITER), desc=\"Collecting experience\"):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "            # Epsilon-greedy action selection (with small epsilon)\n",
        "            if random.random() < 0.05:  # 5% exploration\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    action = best_model(state_tensor).argmax(dim=1).item()\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Store transition\n",
        "            memory.push(\n",
        "                state,\n",
        "                action,\n",
        "                next_state if not done else None,\n",
        "                reward,\n",
        "                done\n",
        "            )\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    # 2. Collect experience against opponent models\n",
        "    for opponent_info in opponents:\n",
        "        opponent = opponent_info[\"model\"]\n",
        "\n",
        "        for episode in tqdm(range(max(1, EPISODES_PER_ITER // 3)), desc=f\"Competing vs {opponent_info['name']}\"):\n",
        "            state, _ = env.reset()\n",
        "            # Store previous state and action for comparison\n",
        "            trajectory = []\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "                # Get action from current model\n",
        "                with torch.no_grad():\n",
        "                    model_action = best_model(state_tensor).argmax(dim=1).item()\n",
        "\n",
        "                # Get action from opponent model\n",
        "                with torch.no_grad():\n",
        "                    opponent_action = opponent(state_tensor).argmax(dim=1).item()\n",
        "\n",
        "                # Execute model's action\n",
        "                next_state, reward, terminated, truncated, _ = env.step(model_action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                # Calculate \"competitive reward\" - bonus if model action matches expert\n",
        "                # This creates a learning signal encouraging better performance\n",
        "                if opponent == expert_model:\n",
        "                    competitive_reward = reward + (1.0 if model_action == opponent_action else -0.1)\n",
        "                else:\n",
        "                    competitive_reward = reward\n",
        "\n",
        "                # Store transition with modified reward\n",
        "                memory.push(\n",
        "                    state,\n",
        "                    model_action,\n",
        "                    next_state if not done else None,\n",
        "                    competitive_reward,\n",
        "                    done\n",
        "                )\n",
        "\n",
        "                # Store opponent's predicted action for this state\n",
        "                trajectory.append((state, opponent_action))\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            # After episode terminates, store expert trajectories with higher priority\n",
        "            if opponent == expert_model:\n",
        "                for s, a in trajectory:\n",
        "                    expert_memory.push(\n",
        "                        s,\n",
        "                        a,\n",
        "                        None,  # Don't need next state for imitation\n",
        "                        0,     # Reward doesn't matter for imitation\n",
        "                        True   # Done flag doesn't matter for imitation\n",
        "                    )\n",
        "\n",
        "    # Save snapshot of current version before training further\n",
        "    torch.save(best_model.state_dict(), f\"models/competition_history/model_iter{iteration+1}_pre.pth\")\n",
        "\n",
        "    # 3. Train on collected experiences with multi-objective loss\n",
        "    if len(memory) >= BATCH_SIZE:\n",
        "        print(f\"Training on {len(memory)} transitions...\")\n",
        "\n",
        "        # Load previous version for regularization\n",
        "        prev_model = DQN(n_observations, n_actions).to(device)\n",
        "        prev_model.load_state_dict(best_model.state_dict())\n",
        "        prev_model.eval()\n",
        "\n",
        "        loss_stats = {'rl': [], 'imitation': [], 'divergence': [], 'safety': [], 'entropy': []}\n",
        "\n",
        "        for step in tqdm(range(500), desc=\"Training\"):  # Train for 500 batches\n",
        "            # Sometimes include expert demonstrations\n",
        "            use_expert = (step % 5 == 0) and len(expert_memory) > BATCH_SIZE\n",
        "\n",
        "            if use_expert:\n",
        "                # Mix regular and expert transitions\n",
        "                reg_transitions = memory.sample(BATCH_SIZE // 2)\n",
        "                exp_transitions = expert_memory.sample(BATCH_SIZE // 2)\n",
        "                transitions = reg_transitions + exp_transitions\n",
        "            else:\n",
        "                transitions = memory.sample(BATCH_SIZE)\n",
        "\n",
        "            batch = Transition(*zip(*transitions))\n",
        "\n",
        "            # Process batch data for training\n",
        "            non_final_mask = torch.tensor(\n",
        "                tuple(map(lambda s: s is not None, batch.next_state)),\n",
        "                device=device, dtype=torch.bool)\n",
        "\n",
        "            non_final_next_states = torch.tensor(\n",
        "                np.array([s for s in batch.next_state if s is not None]),\n",
        "                dtype=torch.float32, device=device)\n",
        "\n",
        "            state_batch = torch.tensor(np.array(batch.state), dtype=torch.float32, device=device)\n",
        "            action_batch = torch.tensor(batch.action, device=device).unsqueeze(1)\n",
        "            reward_batch = torch.tensor(batch.reward, device=device)\n",
        "            done_batch = torch.tensor(batch.done, device=device, dtype=torch.bool)\n",
        "\n",
        "            # Get current Q values\n",
        "            q_values = best_model(state_batch)\n",
        "            state_action_values = q_values.gather(1, action_batch)\n",
        "\n",
        "            # Compute expected Q values with target network\n",
        "            next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "            with torch.no_grad():\n",
        "                next_state_values[non_final_mask] = target_model(non_final_next_states).max(1)[0]\n",
        "\n",
        "            expected_state_action_values = (next_state_values * GAMMA * (~done_batch)) + reward_batch\n",
        "\n",
        "            # Get previous policy logits\n",
        "            with torch.no_grad():\n",
        "                prev_logits = prev_model(state_batch)\n",
        "\n",
        "            # For expert demos, get expert actions for imitation loss\n",
        "            expert_actions = None\n",
        "            if use_expert:\n",
        "                # Only use actions from expert transitions (second half of batch)\n",
        "                expert_actions = action_batch[BATCH_SIZE//2:].squeeze()\n",
        "\n",
        "            # Compute the enhanced composite loss\n",
        "            loss, metrics = compute_enhanced_composite_loss(\n",
        "                state_action_values,\n",
        "                expected_state_action_values.unsqueeze(1),\n",
        "                q_values,\n",
        "                expert_actions,\n",
        "                prev_logits,\n",
        "                state_batch,  # Added state batch parameter\n",
        "                non_final_next_states if non_final_mask.any() else None,  # Added next states parameter\n",
        "                action_batch,  # Added actions parameter\n",
        "                alpha=ALPHA,\n",
        "                beta=BETA\n",
        "            )\n",
        "\n",
        "            # Update loss statistics\n",
        "            for k, v in metrics.items():\n",
        "                loss_stats[k].append(v)\n",
        "\n",
        "            # Optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # Clip gradients to stabilize training\n",
        "            torch.nn.utils.clip_grad_norm_(best_model.parameters(), 10.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Soft update target network\n",
        "            for target_param, local_param in zip(target_model.parameters(), best_model.parameters()):\n",
        "                target_param.data.copy_(TAU * local_param.data + (1.0 - TAU) * target_param.data)\n",
        "\n",
        "            avg_rl = np.mean(loss_stats['rl'])\n",
        "            avg_imt = np.mean(loss_stats['imitation'])\n",
        "            avg_div = np.mean(loss_stats['divergence'])\n",
        "            avg_saf = np.mean(loss_stats['safety'])\n",
        "            avg_ent = np.mean(loss_stats['entropy'])\n",
        "\n",
        "            # *** Added: record these averages into loss_history\n",
        "            loss_history.append({\n",
        "                'rl': avg_rl,\n",
        "                'imitation': avg_imt,\n",
        "                'divergence': avg_div,\n",
        "                'safety': avg_saf,\n",
        "                'entropy': avg_ent\n",
        "            })\n",
        "\n",
        "        # Print average loss components\n",
        "        print(f\"  RL Loss: {np.mean(loss_stats['rl']):.4f}, \" +\n",
        "              f\"Imitation Loss: {np.mean(loss_stats['imitation']):.4f}, \" +\n",
        "              f\"Divergence: {np.mean(loss_stats['divergence']):.4f}, \" +\n",
        "              f\"Safety: {np.mean(loss_stats['safety']):.4f}, \" +\n",
        "              f\"Entropy: {np.mean(loss_stats['entropy']):.4f}\")\n",
        "\n",
        "    # 4. Evaluate against each opponent and against environment\n",
        "    print(\"Evaluating against opponents...\")\n",
        "    competition_results = {}\n",
        "    for opponent_info in opponents:\n",
        "        name = opponent_info[\"name\"]\n",
        "        opponent = opponent_info[\"model\"]\n",
        "        results = evaluate_against_opponent(best_model, opponent, env, episodes=3)\n",
        "        competition_results[name] = results\n",
        "        print(f\"  vs {name}: Win Rate: {results['win_rate']:.2f}, \" +\n",
        "              f\"Avg Reward: {results['model_avg']:.2f} vs {results['opponent_avg']:.2f}\")\n",
        "\n",
        "    # Standard evaluation\n",
        "    current_reward = evaluate_policy(best_model, env)\n",
        "    print(f\"Environment performance: {current_reward:.2f} (best: {best_reward:.2f})\")\n",
        "\n",
        "    # Track performance\n",
        "    performance_history.append({\n",
        "        'iteration': iteration + 1,\n",
        "        'reward': current_reward,\n",
        "        'competition': competition_results\n",
        "    })\n",
        "\n",
        "    # Save if improved\n",
        "    if current_reward > best_reward:\n",
        "        best_reward = current_reward\n",
        "        torch.save(best_model.state_dict(), f\"models/self_play_model_iter{iteration+1}.pth\")\n",
        "        print(f\"  Saved improved model with reward {best_reward:.2f}\")\n",
        "\n",
        "        # Add this version to the pool of competitors for future training\n",
        "        if iteration > 0 and iteration % 3 == 0:  # Add every 3rd improved model\n",
        "            new_competitor = DQN(n_observations, n_actions).to(device)\n",
        "            new_competitor.load_state_dict(best_model.state_dict())\n",
        "            new_competitor.eval()\n",
        "            model_versions.append({\n",
        "                \"model\": new_competitor,\n",
        "                \"name\": f\"SelfPlay-{iteration+1}\"\n",
        "            })\n",
        "            print(f\"Added model version SelfPlay-{iteration+1} to competitor pool\")\n",
        "\n",
        "for h in performance_history:\n",
        "    all_opps.update(h['competition'].keys())\n",
        "opponent_names = sorted(all_opps)\n",
        "\n",
        "# Save final model\n",
        "print(f\"Self-play training complete! Best reward: {best_reward:.2f}\")\n",
        "torch.save(best_model.state_dict(), \"models/enhanced_self_play_best.pth\")\n",
        "\n",
        "# Plot performance over iterations\n",
        "plt.figure(figsize=(12, 6))\n",
        "rewards = [p['reward'] for p in performance_history]\n",
        "plt.plot(range(1, len(rewards)+1), rewards, marker='o')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Average Reward')\n",
        "plt.title('Self-Play Performance Over Time')\n",
        "plt.savefig('self_play_performance.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwY3063EaDq4"
      },
      "outputs": [],
      "source": [
        "iterations = [h['iteration'] for h in performance_history]\n",
        "rewards    = [h['reward']    for h in performance_history]\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(iterations, rewards, marker='o')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Average Reward')\n",
        "plt.title('Self-Play Performance Over Iterations')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AEmhkYoaDdo"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses)\n",
        "plt.xlabel('Updates')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Time')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qicfLWsBaItg"
      },
      "outputs": [],
      "source": [
        "components = ['rl', 'imitation', 'divergence', 'safety', 'entropy']\n",
        "\n",
        "for comp in components:\n",
        "    vals = [lh[comp] for lh in loss_history]\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, len(vals)+1), vals, marker='.')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel(f'{comp.capitalize()} Loss')\n",
        "    plt.title(f'{comp.capitalize()} Loss Over Iterations')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8xVz5d7aIrY"
      },
      "outputs": [],
      "source": [
        "for opp in opponent_names:\n",
        "    iters, win_rates = [], []\n",
        "    for h in performance_history:\n",
        "        if opp in h['competition']:\n",
        "            iters.append(h['iteration'])\n",
        "            win_rates.append(h['competition'][opp]['win_rate'])\n",
        "    if not iters:\n",
        "        continue\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(iters, win_rates, marker='x')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Win Rate')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.title(f'Win Rate vs {opp}')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Pewpf456aIow",
        "outputId": "e6e9b8a6-1024-49f8-f783-b7fc7c18b6d6"
      },
      "outputs": [],
      "source": [
        "for opp in opponent_names:\n",
        "    iters, model_avgs, opp_avgs = [], [], []\n",
        "    for h in performance_history:\n",
        "        if opp in h['competition']:\n",
        "            iters.append(h['iteration'])\n",
        "            model_avgs.append(h['competition'][opp]['model_avg'])\n",
        "            opp_avgs.append(h['competition'][opp]['opponent_avg'])\n",
        "    if not iters:\n",
        "        continue\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(iters, model_avgs, marker='o', label='Model')\n",
        "    plt.plot(iters, opp_avgs,   marker='s', label=opp)\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Average Reward')\n",
        "    plt.title(f'Avg Reward: Model vs {opp}')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIcniACDefyl"
      },
      "outputs": [],
      "source": [
        "for opp in opponent_names:\n",
        "    iters, diffs = [], []\n",
        "    for h in performance_history:\n",
        "        if opp in h['competition']:\n",
        "            iters.append(h['iteration'])\n",
        "            m = h['competition'][opp]['model_avg']\n",
        "            o = h['competition'][opp]['opponent_avg']\n",
        "            diffs.append(m - o)\n",
        "    if not iters:\n",
        "        continue\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(iters, diffs, marker='d')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Reward Difference')\n",
        "    plt.title(f'Reward Difference (Model − {opp})')\n",
        "    plt.axhline(0, linestyle='--')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-I6Jaj6efwN"
      },
      "outputs": [],
      "source": [
        "cummax_rewards = np.maximum.accumulate([h['reward'] for h in performance_history])\n",
        "plt.figure()\n",
        "plt.plot(iterations, cummax_rewards, marker='>')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Best Reward So Far')\n",
        "plt.title('Cumulative Best Reward Over Iterations')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ja5OPTOheft8"
      },
      "outputs": [],
      "source": [
        "window = 3\n",
        "rewards = [h['reward'] for h in performance_history]\n",
        "mov_avg = [np.mean(rewards[max(0, i-window+1):i+1]) for i in range(len(rewards))]\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(iterations, rewards,    marker='o', label='Raw')\n",
        "plt.plot(iterations, mov_avg,    marker='*', label=f'{window}-Itr Moving Avg')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Average Reward')\n",
        "plt.title('Raw vs. Moving Average Reward')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDIY-1hECr6B"
      },
      "source": [
        "## 8. Final Evaluation\n",
        "\n",
        "Compare all trained models to see which performs best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-06T04:49:14.710799Z",
          "iopub.status.idle": "2025-05-06T04:49:14.711072Z",
          "shell.execute_reply": "2025-05-06T04:49:14.710929Z",
          "shell.execute_reply.started": "2025-05-06T04:49:14.710919Z"
        },
        "id": "PB770DK5Cr6C",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Setup for evaluation\n",
        "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "n_observations = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# Load all models to compare\n",
        "models = {\n",
        "    \"Expert DQN\": \"models/dqn_lunar_lander.pth\",\n",
        "    \"Behavioral Cloning\": \"models/imitation_model.pth\",\n",
        "    \"DAgger\": \"models/dagger_model_iter5.pth\",\n",
        "    \"Enhanced Self-Play\": \"models/enhanced_self_play_best.pth\",\n",
        "}\n",
        "\n",
        "# Function to evaluate a model\n",
        "def evaluate_model_with_render(model_path, n_episodes=10):\n",
        "    model = DQN(n_observations, n_actions).to(device)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        model.eval()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load model: {model_path}, error: {e}\")\n",
        "        return []\n",
        "\n",
        "    rewards = []\n",
        "    for i in range(n_episodes):\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                action = model(state_tensor).argmax(dim=1).item()\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "        print(f\"  Episode {i+1}: {total_reward:.2f}\")\n",
        "\n",
        "    return rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2025-05-06T04:49:14.71247Z",
          "iopub.status.idle": "2025-05-06T04:49:14.712799Z",
          "shell.execute_reply": "2025-05-06T04:49:14.712637Z",
          "shell.execute_reply.started": "2025-05-06T04:49:14.712624Z"
        },
        "id": "FYVrLuXRCr6C",
        "outputId": "c4ea7221-b1b5-4ecd-b8a6-35ec8decbf9e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Evaluate all models\n",
        "results = {}\n",
        "for name, path in models.items():\n",
        "    try:\n",
        "        print(f\"Evaluating {name}...\")\n",
        "        rewards = evaluate_model_with_render(path)\n",
        "        results[name] = rewards\n",
        "        if rewards:  # Only calculate if we got valid rewards\n",
        "            print(f\"  Mean reward: {np.mean(rewards):.2f} ± {np.std(rewards):.2f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating {name}: {e}\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-06T04:49:14.7142Z",
          "iopub.status.idle": "2025-05-06T04:49:14.714489Z",
          "shell.execute_reply": "2025-05-06T04:49:14.714392Z",
          "shell.execute_reply.started": "2025-05-06T04:49:14.714378Z"
        },
        "id": "CCa4tsbeCr6C",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Plot results\n",
        "plt.figure(figsize=(12, 6))\n",
        "valid_models = {name: results[name] for name in results if results[name]}\n",
        "plt.boxplot([valid_models[name] for name in valid_models.keys()], labels=list(valid_models.keys()))\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Performance Comparison of Different Models')\n",
        "plt.savefig('model_comparison.png')\n",
        "plt.show()\n",
        "\n",
        "# Print summary\n",
        "print(\"\\nSummary:\")\n",
        "for name in valid_models:\n",
        "    rewards = valid_models[name]\n",
        "    print(f\"{name}: {np.mean(rewards):.2f} ± {np.std(rewards):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "fpTh3iyFgs6T",
        "outputId": "a62bfdaf-30a2-4839-8a22-7191ec860eb9"
      },
      "outputs": [],
      "source": [
        "model_names = list(results.keys())\n",
        "# Compute means and stds\n",
        "means = [np.mean(results[name]) for name in model_names]\n",
        "stds  = [np.std(results[name])  for name in model_names]\n",
        "\n",
        "# 1. Bar chart of mean ± std\n",
        "plt.figure()\n",
        "plt.bar(model_names, means, yerr=stds, capsize=5)\n",
        "plt.ylabel('Average Reward')\n",
        "plt.title('Comparison of Model Performance')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "vX1cte8Ag-kn",
        "outputId": "9a1c24ee-4743-4f3b-f442-3b8cdef4e77e"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.boxplot([results[name] for name in model_names], labels=model_names)\n",
        "plt.ylabel('Reward')\n",
        "plt.title('Reward Distributions Across Models')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "IbHMcdlshBFV",
        "outputId": "9ef3928e-55d4-4ab9-8fa0-35ac3c136b2d"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "for name in model_names:\n",
        "    rewards = results[name]\n",
        "    plt.plot(range(1, len(rewards)+1), rewards, marker='o', label=name)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('Episode Rewards per Model')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYHOFiZ-Cr6C"
      },
      "source": [
        "## 10. Conclusion\n",
        "\n",
        "In this notebook, we've implemented a comprehensive reinforcement learning workflow including:\n",
        "\n",
        "1. Expert DQN training on the LunarLander-v3 environment\n",
        "2. Generation of expert trajectories from the trained DQN model\n",
        "3. Behavioral Cloning to create an initial student model from expert demonstrations\n",
        "4. DAgger (Dataset Aggregation) to improve the student model with additional expert-labeled data\n",
        "5. Enhanced Self-Play reinforcement learning with multi-agent competition dynamics\n",
        "   - Competition between different model versions\n",
        "   - Multi-objective optimization (imitation, competition, and policy regularization)\n",
        "   - Generation-based model pool for diverse opponents\n",
        "7. Comprehensive evaluation of all approaches\n",
        "\n",
        "This combined approach leverages imitation learning (learning from expert demonstrations), reinforcement learning (learning from experience), and model compression techniques to create a robust, efficient agent that can effectively solve the lunar landing task while maintaining a small model size.\n",
        "\n",
        "Our enhanced self-play mechanism adapted concepts from multi-agent training to a single-agent environment by creating a competitive dynamic between different versions of our policy, leading to more robust performance."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31011,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0195f6a3f23c4412b9833d45f8bab700": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c9bf07b0078468fa833ab9111784007": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17267cd5c7e2463693ef42d8353dfbce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23cc1890aa204a38872648920d4c4b6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31d602e4e17b4cbdaec861c4d3b62afa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cd00882062241d6b652abcec5761655": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ce8a41694cf43fdb5dd5f3ed2745edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "402e82e6276444e6910089f5af342f7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44923cd3136b4079933948325ea49512",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66a053ea81f6415995880111cf2b3fca",
            "value": 200
          }
        },
        "44923cd3136b4079933948325ea49512": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e447f30d60d4be8b27fb8a35ed62b77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56cdbf5594524beeacf154057f3fb53a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23cc1890aa204a38872648920d4c4b6f",
            "placeholder": "​",
            "style": "IPY_MODEL_0195f6a3f23c4412b9833d45f8bab700",
            "value": " 800/800 [23:05&lt;00:00,  1.63s/it]"
          }
        },
        "5def1efbc04449e0a055b996d075f20a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82ed5c7f2f17497f9adaf94abe3beaf4",
            "placeholder": "​",
            "style": "IPY_MODEL_17267cd5c7e2463693ef42d8353dfbce",
            "value": "BC Training: 100%"
          }
        },
        "6359400387974a5798293f21df4ba5ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66a053ea81f6415995880111cf2b3fca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82ed5c7f2f17497f9adaf94abe3beaf4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b8fde6236924faa88e5c7338c4a0696": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5def1efbc04449e0a055b996d075f20a",
              "IPY_MODEL_402e82e6276444e6910089f5af342f7b",
              "IPY_MODEL_9d29573c53e44d0983b855c1567bacf3"
            ],
            "layout": "IPY_MODEL_6359400387974a5798293f21df4ba5ad"
          }
        },
        "9d29573c53e44d0983b855c1567bacf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f868ca4da37f4b5091f37b51a45ba3d3",
            "placeholder": "​",
            "style": "IPY_MODEL_0c9bf07b0078468fa833ab9111784007",
            "value": " 200/200 [00:57&lt;00:00,  3.49it/s]"
          }
        },
        "bbd5c1788f3543518c7b4c65cd9518a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d00982750c094b32a3a5c64be2aec880": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4bcc5a44fbe4e198a1ffa0e33a495cf",
              "IPY_MODEL_fbf2362d237e4646a9e988702d765d15",
              "IPY_MODEL_56cdbf5594524beeacf154057f3fb53a"
            ],
            "layout": "IPY_MODEL_31d602e4e17b4cbdaec861c4d3b62afa"
          }
        },
        "f4bcc5a44fbe4e198a1ffa0e33a495cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cd00882062241d6b652abcec5761655",
            "placeholder": "​",
            "style": "IPY_MODEL_3ce8a41694cf43fdb5dd5f3ed2745edc",
            "value": "Training Episodes: 100%"
          }
        },
        "f868ca4da37f4b5091f37b51a45ba3d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbf2362d237e4646a9e988702d765d15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbd5c1788f3543518c7b4c65cd9518a1",
            "max": 800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e447f30d60d4be8b27fb8a35ed62b77",
            "value": 800
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
