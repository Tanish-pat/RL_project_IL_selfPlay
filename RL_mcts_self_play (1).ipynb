{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "JX4KOHmaWAEB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o9L68X3oBaxF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e46e0b8e-89b0-49f4-f929-2b3f698c5aeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y swig\n",
        "\n",
        "!git clone https://github.com/openai/box2d-py\n",
        "%cd box2d-py\n",
        "!pip install -e .\n",
        "\n",
        "%cd ..\n",
        "!pip install gymnasium[box2d] --no-deps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQGMU56Ewh7d",
        "outputId": "c55e4967-a1a3-43b6-d08e-7e44b4be69ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "fatal: destination path 'box2d-py' already exists and is not an empty directory.\n",
            "/content/box2d-py\n",
            "Obtaining file:///content/box2d-py\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: box2d-py\n",
            "  Attempting uninstall: box2d-py\n",
            "    Found existing installation: box2d-py 2.3.8\n",
            "    Uninstalling box2d-py-2.3.8:\n",
            "      Successfully uninstalled box2d-py-2.3.8\n",
            "  Running setup.py develop for box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "/content\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# importing the model"
      ],
      "metadata": {
        "id": "EEjSm-dBwr1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Our DQN architecture\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(n_observations, 128)\n",
        "        self.layer2 = nn.Linear(128, 128)\n",
        "        self.layer3 = nn.Linear(128, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.layer3(x)\n",
        "\n",
        "# Load the environment\n",
        "env = gym.make(\"LunarLander-v3\")\n",
        "\n",
        "n_observations = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# Loading model, we are loading 5 dagger models to see how dagger performed during many instances in the codes given before\n",
        "model1 = DQN(n_observations, n_actions).to(device)\n",
        "model2 = DQN(n_observations, n_actions).to(device)\n",
        "model3 = DQN(n_observations, n_actions).to(device)\n",
        "model4 = DQN(n_observations, n_actions).to(device)\n",
        "model5 = DQN(n_observations, n_actions).to(device)\n",
        "model1.load_state_dict(torch.load(\"/content/dagger_model_iter1.pth\"))  # replace with your actual path\n",
        "model1.eval()\n",
        "model2.load_state_dict(torch.load(\"/content/dagger_model_iter2.pth\"))  # replace with your actual path\n",
        "model2.eval()\n",
        "model3.load_state_dict(torch.load(\"/content/dagger_model_iter3.pth\"))  # replace with your actual path\n",
        "model3.eval()\n",
        "model4.load_state_dict(torch.load(\"/content/dagger_model_iter4.pth\"))  # replace with your actual path\n",
        "model4.eval()\n",
        "model5.load_state_dict(torch.load(\"/content/dagger_model_iter5.pth\"))  # replace with your actual path\n",
        "model5.eval()\n",
        "\n",
        "# Evaluation loop\n",
        "def evaluate_policy(env, model, episodes=100):\n",
        "    total_rewards = []\n",
        "    for ep in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0.0\n",
        "        while not done:\n",
        "            obs_tensor = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                q_values = model(obs_tensor)\n",
        "                action = torch.argmax(q_values, dim=1).item()\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "        total_rewards.append(total_reward)\n",
        "\n",
        "    avg_reward = np.mean(total_rewards)\n",
        "    print(f\"Average Reward over {episodes} episodes: {avg_reward:.2f}\")\n",
        "    return avg_reward\n",
        "\n",
        "evaluate_policy(env, model1)\n",
        "evaluate_policy(env, model2)\n",
        "evaluate_policy(env, model3)\n",
        "evaluate_policy(env, model4)\n",
        "evaluate_policy(env, model5)\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOYJx5PFwxMW",
        "outputId": "21356b95-e990-441f-fe68-694b78128c0d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Reward over 100 episodes: 242.07\n",
            "Average Reward over 100 episodes: 246.25\n",
            "Average Reward over 100 episodes: 229.65\n",
            "Average Reward over 100 episodes: 236.86\n",
            "Average Reward over 100 episodes: 237.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self play with MCTS\n",
        "\n",
        "We have implemented self play a little differently. Common self play is covered in two player competitive environments, but we are doing a single agent self play mechanism."
      ],
      "metadata": {
        "id": "0wbttWKcjb5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full modified MCTS self-play code with visualization and video recording\n",
        "\n",
        "\n",
        "from collections import deque\n",
        "import copy\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(n_observations, 128)\n",
        "        self.layer2 = nn.Linear(128, 128)\n",
        "        self.layer3 = nn.Linear(128, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.layer3(x)\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, state, parent=None, action=None):\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.action = action\n",
        "        self.children = []\n",
        "        self.visits = 0\n",
        "        self.total_value = 0.0\n",
        "        self.mean_value = 0.0\n",
        "        self.prior = 0.0\n",
        "        self.virtual_loss = 0\n",
        "        self.terminal = False\n",
        "\n",
        "    def is_fully_expanded(self):\n",
        "        return len(self.children) > 0 and all(child.visits > 0 for child in self.children)\n",
        "\n",
        "    def best_child(self, cpuct=1.0):\n",
        "        if len(self.children) == 0:\n",
        "            return None\n",
        "        values = np.array([child.mean_value for child in self.children])\n",
        "        if len(np.unique(values)) > 1:\n",
        "            norm_values = 2 * (values - np.min(values)) / (np.max(values) - np.min(values)) - 1\n",
        "        else:\n",
        "            norm_values = np.ones_like(values)\n",
        "        scores = norm_values + cpuct * np.array([child.prior * np.sqrt(self.visits) / (1 + child.visits) for child in self.children])\n",
        "        return self.children[np.argmax(scores)]\n",
        "\n",
        "    def expand(self, action_probs, env):\n",
        "        for action, prob in enumerate(action_probs):\n",
        "            if prob > 0:\n",
        "                env_copy = copy.deepcopy(env)  # Still deepcopy the env to avoid issues\n",
        "                obs, _ = env_copy.reset()       # Reset to get a fresh lander object\n",
        "                env_copy.unwrapped.state = self.state.copy() if hasattr(self.state, 'copy') else self.state\n",
        "                next_state, reward, terminated, truncated, _ = env_copy.step(action)  # Now step should work\n",
        "                done = terminated or truncated\n",
        "                child = Node(next_state, self, action)\n",
        "                child.prior = prob\n",
        "                child.terminal = done\n",
        "                self.children.append(child)\n",
        "\n",
        "    def update(self, value):\n",
        "        self.visits += 1\n",
        "        self.total_value += value\n",
        "        self.mean_value = self.total_value / self.visits\n",
        "\n",
        "    def update_virtual_loss(self, virtual_loss_weight=0.01):\n",
        "        self.virtual_loss += virtual_loss_weight * abs(self.mean_value)\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self, policy_net, env, cpuct=1.0, num_simulations=10):\n",
        "        self.policy_net = policy_net\n",
        "        self.env = env\n",
        "        self.cpuct = cpuct\n",
        "        self.num_simulations = num_simulations\n",
        "        self.virtual_loss_weight = 0.01\n",
        "\n",
        "    def search(self, root_state):\n",
        "        root = Node(root_state)\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(root_state).unsqueeze(0).to(device)\n",
        "            action_probs = torch.softmax(self.policy_net(state_tensor), dim=1).cpu().numpy().flatten()\n",
        "        root.expand(action_probs, self.env)\n",
        "\n",
        "        for _ in range(self.num_simulations):\n",
        "            node = root\n",
        "            search_path = [node]\n",
        "            while node.is_fully_expanded() and not node.terminal:\n",
        "                node = node.best_child(self.cpuct)\n",
        "                node.update_virtual_loss(self.virtual_loss_weight)\n",
        "                search_path.append(node)\n",
        "            if not node.terminal:\n",
        "                with torch.no_grad():\n",
        "                    state_tensor = torch.FloatTensor(node.state).unsqueeze(0).to(device)\n",
        "                    action_probs = torch.softmax(self.policy_net(state_tensor), dim=1).cpu().numpy().flatten()\n",
        "                node.expand(action_probs, self.env)\n",
        "                if node.children:\n",
        "                    node = random.choice([c for c in node.children if c.visits == 0])\n",
        "                    search_path.append(node)\n",
        "            value = 0 if node.terminal else self.rollout(node.state)\n",
        "            for node in reversed(search_path):\n",
        "                node.update(value)\n",
        "                node.virtual_loss = 0\n",
        "        return root\n",
        "\n",
        "    def rollout(self, state):\n",
        "        env_copy = copy.deepcopy(self.env)\n",
        "        _, _ = env_copy.reset()\n",
        "        env_copy.unwrapped.state = state.copy() if hasattr(state, 'copy') else state\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for _ in range(30):  # shorten for speed\n",
        "            with torch.no_grad():\n",
        "                state_tensor = torch.FloatTensor(env_copy.unwrapped.state).unsqueeze(0).to(device)\n",
        "                action_probs = torch.softmax(self.policy_net(state_tensor), dim=1).cpu().numpy().flatten()\n",
        "            action = np.random.choice(len(action_probs), p=action_probs)\n",
        "            next_state, reward, terminated, truncated, _ = env_copy.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return total_reward\n",
        "\n",
        "    def get_action(self, state, temperature=1.0):\n",
        "        root = self.search(state)\n",
        "        visit_counts = np.array([child.visits for child in root.children])\n",
        "        actions = [child.action for child in root.children]\n",
        "        if temperature == 0:\n",
        "            return actions[np.argmax(visit_counts)]\n",
        "        visit_probs = visit_counts ** (1 / temperature)\n",
        "        visit_probs /= np.sum(visit_probs)\n",
        "        return np.random.choice(actions, p=visit_probs)\n",
        "\n",
        "class PolicyIterationTrainer:\n",
        "    def __init__(self, env_name=\"LunarLander-v3\", num_generations=5, games_per_gen=10, num_simulations=10,\n",
        "                 buffer_size=50000, batch_size=64, lr=1e-4):\n",
        "        self.env_name = env_name\n",
        "        self.env = gym.make(env_name)\n",
        "        self.num_generations = num_generations\n",
        "        self.games_per_gen = games_per_gen\n",
        "        self.num_simulations = num_simulations\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "\n",
        "        n_obs = self.env.observation_space.shape[0]\n",
        "        n_actions = self.env.action_space.n\n",
        "        self.policy_net = DQN(n_obs, n_actions).to(device)\n",
        "        self.target_net = DQN(n_obs, n_actions).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.replay_buffer = deque(maxlen=buffer_size)\n",
        "        self.mcts = MCTS(self.policy_net, self.env, num_simulations=num_simulations)\n",
        "\n",
        "        self.loss_history = []\n",
        "        self.eval_history = []\n",
        "        self.action_log = []\n",
        "\n",
        "    def generate_self_play_data(self):\n",
        "        for _ in range(self.games_per_gen):\n",
        "            state, _ = self.env.reset()\n",
        "            done = False\n",
        "            episode_data = []\n",
        "            while not done:\n",
        "                action = self.mcts.get_action(state)\n",
        "                self.action_log.append(action)\n",
        "                episode_data.append((state.copy(), action))\n",
        "                state, _, terminated, truncated, _ = self.env.step(action)\n",
        "                done = terminated or truncated\n",
        "            self.replay_buffer.extend(episode_data)\n",
        "\n",
        "    def train_policy(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return None\n",
        "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
        "        states, actions = zip(*batch)\n",
        "        states = torch.FloatTensor(np.array(states)).to(device)\n",
        "        actions = torch.LongTensor(actions).to(device)\n",
        "        logits = self.policy_net(states)\n",
        "        loss = F.cross_entropy(logits, actions)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def evaluate(self, episodes=5):\n",
        "        total_reward = 0\n",
        "        self.policy_net.eval()\n",
        "        for _ in range(episodes):\n",
        "            state, _ = self.env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                with torch.no_grad():\n",
        "                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "                    action = torch.argmax(self.policy_net(state_tensor)).item()\n",
        "                state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "                done = terminated or truncated\n",
        "                total_reward += reward\n",
        "        self.policy_net.train()\n",
        "        return total_reward / episodes\n",
        "\n",
        "    def plot_visuals(self):\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.loss_history)\n",
        "        plt.title(\"Training Loss\")\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(self.eval_history)\n",
        "        plt.title(\"Evaluation Reward\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"training_curves.png\")\n",
        "        plt.close()\n",
        "\n",
        "        sns.countplot(x=self.action_log)\n",
        "        plt.title(\"Action Distribution\")\n",
        "        plt.savefig(\"action_dist.png\")\n",
        "        plt.close()\n",
        "\n",
        "    def record_policy_video(self, name):\n",
        "        env = gym.make(self.env_name, render_mode=\"rgb_array\")\n",
        "        env = gym.wrappers.RecordVideo(env, video_folder=\"videos\", name_prefix=name)\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            with torch.no_grad():\n",
        "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "                action = torch.argmax(self.policy_net(state_tensor)).item()\n",
        "            state, _, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "        env.close()\n",
        "\n",
        "    def train(self):\n",
        "        os.makedirs(\"videos\", exist_ok=True)\n",
        "        self.record_policy_video(\"initial_policy\")\n",
        "\n",
        "        best_reward = -float(\"inf\")\n",
        "        for g in range(self.num_generations):\n",
        "            self.generate_self_play_data()\n",
        "            loss = self.train_policy()\n",
        "            reward = self.evaluate()\n",
        "            if loss is not None:\n",
        "                self.loss_history.append(loss)\n",
        "            self.eval_history.append(reward)\n",
        "            print(f\"Gen {g+1}: Loss={loss:.4f} | Eval={reward:.2f}\")\n",
        "            if reward > best_reward:\n",
        "                best_reward = reward\n",
        "                torch.save(self.policy_net.state_dict(), \"best_policy_net.pth\")\n",
        "                print(\">> Best model saved.\")\n",
        "            if g % 5 == 0:\n",
        "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        self.plot_visuals()\n",
        "        self.record_policy_video(\"final_policy\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    trainer = PolicyIterationTrainer()\n",
        "    try:\n",
        "        trainer.policy_net.load_state_dict(torch.load(\"/content/dagger_model_iter2.pth\"))\n",
        "        trainer.target_net.load_state_dict(torch.load(\"/content/dagger_model_iter2.pth\"))\n",
        "        print(\"Loaded pre-trained model.\")\n",
        "    except:\n",
        "        print(\"No pre-trained model found.\")\n",
        "    trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXnQxtDXbDXP",
        "outputId": "a486c3c9-faa1-49d2-cdcf-70e60c548dea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pre-trained model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gen 1: Loss=40.3473 | Eval=214.49\n",
            ">> Best model saved.\n",
            "Gen 2: Loss=37.5454 | Eval=232.15\n",
            ">> Best model saved.\n",
            "Gen 3: Loss=28.6853 | Eval=240.97\n",
            ">> Best model saved.\n",
            "Gen 4: Loss=33.0428 | Eval=252.18\n",
            ">> Best model saved.\n",
            "Gen 5: Loss=36.7560 | Eval=216.21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uiUsO4dfksWQ"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}